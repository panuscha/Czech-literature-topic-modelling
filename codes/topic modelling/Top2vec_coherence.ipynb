{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IMPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.corpora import Dictionary\n",
    "from csv import writer\n",
    "from top2vec import Top2Vec\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LOAD BOOKS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_books_chunks_from_document(CONST, file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "\n",
    "    books = []\n",
    "    books_info = []\n",
    "    start_tag = '<doc title=\"'\n",
    "    end_tag = '</doc>'\n",
    "    start_index = 0\n",
    "\n",
    "    while True:\n",
    "        book_start = content.find(start_tag, start_index)\n",
    "        if book_start == -1:\n",
    "            break\n",
    "\n",
    "        book_end = content.find(end_tag, book_start)\n",
    "        if book_end == -1:\n",
    "            break\n",
    "\n",
    "        book_text = content[book_start:book_end + len(end_tag)]\n",
    "        book_info = book_text.strip()[5:book_text.index('>') + 1]  # Remove '<doc' and '</doc>'\n",
    "\n",
    "        book_info_list = book_info.split('\" ')\n",
    "        book_info_dict = {}\n",
    "\n",
    "        for item in book_info_list:\n",
    "            key, value = item.split('=')\n",
    "            book_info_dict[key.strip()] = value.strip('\"')\n",
    "\n",
    "        book_content = book_text[book_text.index('>') + 1:-len(end_tag)].strip()\n",
    "        book_content = book_content.split(' ')\n",
    "        length = len(book_content)\n",
    "        for i in range(math.ceil(length/CONST)):\n",
    "            i = i*CONST\n",
    "            end = i+CONST if i+CONST < length-1 else length-1\n",
    "            books.append(\" \".join(book_content[i:end]))\n",
    "            books_info.append(book_info_dict)\n",
    "\n",
    "        start_index = book_end + len(end_tag)\n",
    "\n",
    "    return books, books_info\n",
    "\n",
    "\n",
    "def load_books_from_document(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "\n",
    "    books = []\n",
    "    books_info = []\n",
    "    start_tag = '<doc title=\"'\n",
    "    end_tag = '</doc>'\n",
    "    start_index = 0\n",
    "\n",
    "    while True:\n",
    "        book_start = content.find(start_tag, start_index)\n",
    "        if book_start == -1:\n",
    "            break\n",
    "\n",
    "        book_end = content.find(end_tag, book_start)\n",
    "        if book_end == -1:\n",
    "            break\n",
    "\n",
    "        book_text = content[book_start:book_end + len(end_tag)]\n",
    "        book_info = book_text.strip()[5:book_text.index('>') + 1]  # Remove '<doc' and '</doc>'  # Remove '<doc' and '</doc>'\n",
    "\n",
    "        book_info_list = book_info.split('\" ')\n",
    "        book_info_dict = {}\n",
    "\n",
    "        for item in book_info_list:\n",
    "            key, value = item.split('=')\n",
    "            book_info_dict[key.strip()] = value.strip('\"')\n",
    "\n",
    "        book_content = book_text[book_text.index('>') + 1:-len(end_tag)].strip()        \n",
    "        books.append(book_content)\n",
    "\n",
    "        books_info.append(book_info_dict)\n",
    "\n",
    "        start_index = book_end + len(end_tag)\n",
    "\n",
    "    return books, books_info\n",
    "\n",
    "\n",
    "\n",
    "def load_books_blocks_from_document(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "\n",
    "    books = []\n",
    "    books_info = []\n",
    "    start_tag = '<doc title=\"'\n",
    "    end_tag = '</doc>'\n",
    "    start_index = 0\n",
    "\n",
    "    while True:\n",
    "        book_start = content.find(start_tag, start_index)\n",
    "        if book_start == -1:\n",
    "            break\n",
    "\n",
    "        book_end = content.find(end_tag, book_start)\n",
    "        if book_end == -1:\n",
    "            break\n",
    "\n",
    "        book_text = content[book_start:book_end + len(end_tag)]\n",
    "        book_info = book_text.strip()[5:book_text.index('>') + 1]  # Remove '<doc' and '</doc>'  # Remove '<doc' and '</doc>'\n",
    "\n",
    "        book_info_list = book_info.split('\" ')\n",
    "        book_info_dict = {}\n",
    "\n",
    "        for item in book_info_list:\n",
    "            key, value = item.split('=')\n",
    "            book_info_dict[key.strip()] = value.strip('\"')\n",
    "\n",
    "        book_content = book_text[book_text.index('>') + 1:-len(end_tag)].strip()\n",
    "        book_content = book_content.split('\\n')\n",
    "        for book_block in book_content:\n",
    "            books.append(book_block)\n",
    "            books_info.append(book_info_dict)\n",
    "\n",
    "        start_index = book_end + len(end_tag)\n",
    "\n",
    "    return books, books_info\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TOPIC COHERENCE SCORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of topics:  61\n",
      "Number of topics:  5\n",
      "Topic Coherence Score: 0.4944872879976783\n",
      "Number of topics:  10\n",
      "Topic Coherence Score: 0.4417502225503015\n",
      "Number of topics:  20\n",
      "Topic Coherence Score: 0.44478408784843076\n",
      "Number of topics:  30\n",
      "Topic Coherence Score: 0.42588145785895165\n",
      "Number of topics:  61\n",
      "Topic Coherence Score: 0.41091343826656324\n",
      "Number of topics:  76\n",
      "Number of topics:  5\n",
      "Topic Coherence Score: 0.38490202226310133\n",
      "Number of topics:  10\n",
      "Topic Coherence Score: 0.38467773855381304\n",
      "Number of topics:  20\n",
      "Topic Coherence Score: 0.3841448364774397\n",
      "Number of topics:  30\n",
      "Topic Coherence Score: 0.3900326360022047\n",
      "Number of topics:  76\n",
      "Topic Coherence Score: 0.40496879995123164\n",
      "Number of topics:  93\n",
      "Number of topics:  5\n",
      "Topic Coherence Score: 0.4335416382679271\n",
      "Number of topics:  10\n",
      "Topic Coherence Score: 0.4247315894192266\n",
      "Number of topics:  20\n",
      "Topic Coherence Score: 0.4064873652164122\n",
      "Number of topics:  30\n",
      "Topic Coherence Score: 0.40947265281006334\n",
      "Number of topics:  93\n",
      "Topic Coherence Score: 0.3962974698384906\n"
     ]
    }
   ],
   "source": [
    "path = \"data/lem\"\n",
    "year = \"1990_2019\"\n",
    "category = \"all\"\n",
    "file_name = year+\"_\"+category+\"_no_names_beletrie\"\n",
    "file_path = path+\"/\"+file_name + \".txt\"\n",
    "chunk = 2000\n",
    "divide = \"chunk\"\n",
    "\n",
    "\n",
    "for year in [\"1990_1999\", \"2000_2009\", \"2010_2019\"]:\n",
    "    for divide in [\"chunk\"]: \n",
    "            for chunk in [2000]:\n",
    "                file_name = year+\"_\"+category+\"_no_names_beletrie_with_stopwords\"\n",
    "                file_path = path+\"/\"+file_name + \".txt\"\n",
    "                if divide == \"chunk\":\n",
    "                    books, books_info = load_books_chunks_from_document(chunk, file_path)\n",
    "                    d = str(chunk) + \" \" + divide\n",
    "                    save_path = \"data/models/{divide}/top2vec__with_stopwords\".format(divide = d) + file_name\n",
    "                else:  \n",
    "                    file_name = year+\"_\"+category+\"_no_names_beletrie\"\n",
    "                    save_path = \"data/models/{divide}/top2vec__with_stopwords\".format(divide = \"blocks\") + file_name    \n",
    "                    \n",
    "                    file_name = year+\"_\"+category+\"_no_names_blocks_beletrie\"\n",
    "                    file_path = path+\"/\"+file_name + \".txt\" \n",
    "                    books, books_info = load_books_blocks_from_document(file_path)\n",
    "                     \n",
    "\n",
    "                top2vec_model = Top2Vec.load(save_path)\n",
    "\n",
    "                topic_sizes, topic_nums = top2vec_model.get_topic_sizes()\n",
    "\n",
    "                num_topics = top2vec_model.get_num_topics()\n",
    "\n",
    "                print(\"Number of topics: \", num_topics)\n",
    "\n",
    "                original_num_topics = num_topics\n",
    "\n",
    "                for num_topics in [5, 10, 20, 30,  original_num_topics]:\n",
    "\n",
    "                #for topic_size, topic_num in zip(topic_sizes[:num_topics], topic_nums[:num_topics]):\n",
    "                #    print(f\"Topic Num {topic_num} has {topic_size} documents.\")\n",
    "                    if num_topics == original_num_topics: \n",
    "                        reduced = False\n",
    "                    else:\n",
    "                        reduced = True    \n",
    "                        ret = top2vec_model.hierarchical_topic_reduction(num_topics=num_topics)\n",
    "\n",
    "                    print(\"Number of topics: \", top2vec_model.get_num_topics(reduced=reduced))\n",
    "\n",
    "                    # Get the topics and their document clusters\n",
    "                    topic_words, word_scores, topics = top2vec_model.get_topics(num_topics=num_topics, reduced=reduced) \n",
    "\n",
    "                    topic_sizes, topic_nums = top2vec_model.get_topic_sizes(reduced=reduced)\n",
    "\n",
    "                    # Tokenize the documents\n",
    "                    tokenized_data = [word_tokenize(doc) for doc in books]\n",
    "\n",
    "                    dictionary = Dictionary(tokenized_data)\n",
    "\n",
    "                    # Calculate topic coherence using Gensim's CoherenceModel\n",
    "                    coherence_model = CoherenceModel(\n",
    "                        topics=topic_words,\n",
    "                        texts=tokenized_data,\n",
    "                        dictionary=dictionary,\n",
    "                        coherence='c_v'  # You can use other coherence measures as well\n",
    "                    )\n",
    "                    coherence_score = coherence_model.get_coherence()\n",
    "\n",
    "                    print(\"Topic Coherence Score:\", coherence_score)\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
