{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from top2vec import Top2Vec\n",
    "import pandas as pd\n",
    "from csv import writer\n",
    "import umap\n",
    "import umap.plot\n",
    "import matplotlib.pyplot as plt\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"data/lem\"\n",
    "year = \"2010_2019\"\n",
    "category = \"all\"\n",
    "file_name = year+\"_\"+category+\"_no_names_beletrie\"\n",
    "file_path = path+\"/\"+file_name + \".txt\"\n",
    "chunk = 2000\n",
    "divide = \"chunk\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dokuments\n",
    "\n",
    "Three options - chunks, blocks and the whole document "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_books_chunks_from_document(CONST, file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "\n",
    "    books = []\n",
    "    books_info = []\n",
    "    start_tag = '<doc title=\"'\n",
    "    end_tag = '</doc>'\n",
    "    start_index = 0\n",
    "\n",
    "    while True:\n",
    "        book_start = content.find(start_tag, start_index)\n",
    "        if book_start == -1:\n",
    "            break\n",
    "\n",
    "        book_end = content.find(end_tag, book_start)\n",
    "        if book_end == -1:\n",
    "            break\n",
    "\n",
    "        book_text = content[book_start:book_end + len(end_tag)]\n",
    "        book_info = book_text.strip()[5:book_text.index('>') + 1]  # Remove '<doc' and '</doc>'\n",
    "\n",
    "        book_info_list = book_info.split('\" ')\n",
    "        book_info_dict = {}\n",
    "\n",
    "        for item in book_info_list:\n",
    "            key, value = item.split('=')\n",
    "            book_info_dict[key.strip()] = value.strip('\"')\n",
    "\n",
    "        book_content = book_text[book_text.index('>') + 1:-len(end_tag)].strip()\n",
    "        book_content = book_content.split(' ')\n",
    "        length = len(book_content)\n",
    "        for i in range(math.ceil(length/CONST)):\n",
    "            i = i*CONST\n",
    "            end = i+CONST if i+CONST < length-1 else length-1\n",
    "            books.append(\" \".join(book_content[i:end]))\n",
    "            books_info.append(book_info_dict)\n",
    "\n",
    "        start_index = book_end + len(end_tag)\n",
    "\n",
    "    return books, books_info\n",
    "\n",
    "\n",
    "def load_books_from_document(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "\n",
    "    books = []\n",
    "    books_info = []\n",
    "    start_tag = '<doc title=\"'\n",
    "    end_tag = '</doc>'\n",
    "    start_index = 0\n",
    "\n",
    "    while True:\n",
    "        book_start = content.find(start_tag, start_index)\n",
    "        if book_start == -1:\n",
    "            break\n",
    "\n",
    "        book_end = content.find(end_tag, book_start)\n",
    "        if book_end == -1:\n",
    "            break\n",
    "\n",
    "        book_text = content[book_start:book_end + len(end_tag)]\n",
    "        book_info = book_text.strip()[5:book_text.index('>') + 1]  # Remove '<doc' and '</doc>'  # Remove '<doc' and '</doc>'\n",
    "\n",
    "        book_info_list = book_info.split('\" ')\n",
    "        book_info_dict = {}\n",
    "\n",
    "        for item in book_info_list:\n",
    "            key, value = item.split('=')\n",
    "            book_info_dict[key.strip()] = value.strip('\"')\n",
    "\n",
    "        book_content = book_text[book_text.index('>') + 1:-len(end_tag)].strip()        \n",
    "        books.append(book_content)\n",
    "\n",
    "        books_info.append(book_info_dict)\n",
    "\n",
    "        start_index = book_end + len(end_tag)\n",
    "\n",
    "    return books, books_info\n",
    "\n",
    "\n",
    "\n",
    "def load_books_blocks_from_document(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "\n",
    "    books = []\n",
    "    books_info = []\n",
    "    start_tag = '<doc title=\"'\n",
    "    end_tag = '</doc>'\n",
    "    start_index = 0\n",
    "\n",
    "    while True:\n",
    "        book_start = content.find(start_tag, start_index)\n",
    "        if book_start == -1:\n",
    "            break\n",
    "\n",
    "        book_end = content.find(end_tag, book_start)\n",
    "        if book_end == -1:\n",
    "            break\n",
    "\n",
    "        book_text = content[book_start:book_end + len(end_tag)]\n",
    "        book_info = book_text.strip()[5:book_text.index('>') + 1]  # Remove '<doc' and '</doc>'  # Remove '<doc' and '</doc>'\n",
    "\n",
    "        book_info_list = book_info.split('\" ')\n",
    "        book_info_dict = {}\n",
    "\n",
    "        for item in book_info_list:\n",
    "            key, value = item.split('=')\n",
    "            book_info_dict[key.strip()] = value.strip('\"')\n",
    "\n",
    "        book_content = book_text[book_text.index('>') + 1:-len(end_tag)].strip()\n",
    "        book_content = book_content.split('\\n')\n",
    "        for book_block in book_content:\n",
    "            books.append(book_block)\n",
    "            books_info.append(book_info_dict)\n",
    "\n",
    "        start_index = book_end + len(end_tag)\n",
    "\n",
    "    return books, books_info\n",
    "\n",
    "\n",
    "if divide == \"chunk\":\n",
    "    books, books_info = load_books_chunks_from_document(chunk, file_path)\n",
    "else:    \n",
    "    books, books_info = load_books_blocks_from_document(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if divide == \"chunk\":\n",
    "    d = str(chunk) + \" \" + divide\n",
    "    save_path = \"data/models/{divide}/top2vec_\".format(divide = d) + file_name\n",
    "else:\n",
    "    save_path = \"data/models/{divide}/top2vec_\".format(divide = \"blocks\") + file_name    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save books info "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# with open('data\\\\books_info_{date}.csv'.format(date = year), 'w', newline='') as f:\n",
    "#     writer = csv.writer(f)\n",
    "#     writer.writerows(books_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create top2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create the Top2Vec model\n",
    "top2vec_model = Top2Vec(documents=books)\n",
    "\n",
    "top2vec_model.save(save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top2vec_model = Top2Vec.load(save_path)\n",
    "\n",
    "topic_sizes, topic_nums = top2vec_model.get_topic_sizes()\n",
    "\n",
    "num_topics = top2vec_model.get_num_topics()\n",
    "\n",
    "print(\"Number of topics: \", num_topics)\n",
    "\n",
    "original_num_topics = num_topics\n",
    "\n",
    "#for topic_size, topic_num in zip(topic_sizes[:num_topics], topic_nums[:num_topics]):\n",
    "#    print(f\"Topic Num {topic_num} has {topic_size} documents.\")\n",
    "\n",
    "num_topics = 10\n",
    "reduced = True\n",
    "ret = top2vec_model.hierarchical_topic_reduction(num_topics=num_topics)\n",
    "\n",
    "print(\"Number of topics: \", top2vec_model.get_num_topics(reduced=reduced))\n",
    "\n",
    "print(ret)\n",
    "topic_sizes, topic_nums = top2vec_model.get_topic_sizes(reduced=reduced)\n",
    "for topic_size, topic_num in zip(topic_sizes[:num_topics], topic_nums[:num_topics]):\n",
    "    print(f\"Topic Num {topic_num} has {topic_size} documents.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the topics and their document clusters\n",
    "topic_words, word_scores, topics = top2vec_model.get_topics(num_topics=num_topics, reduced=reduced) # Specify the number of topics you want\n",
    "\n",
    "topic_number = 0\n",
    "\n",
    "for words, scores, num in zip(topic_words[topic_number:], word_scores[topic_number:], topics[topic_number:]):\n",
    "    print(f\"Topic {num}\")\n",
    "    for word, score in zip(words, scores):\n",
    "        print(word, score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents, document_scores, document_ids = top2vec_model.search_documents_by_topic(topic_num=1, num_docs=10, reduced=reduced)\n",
    "\n",
    "for doc, score, doc_id in zip(documents, document_scores, document_ids):\n",
    "    print(f\"Document: {doc_id}, Score: {score}\")\n",
    "    print(\"-----------\")\n",
    "    print(doc)\n",
    "    print(\"-----------\")\n",
    "    print()\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DF from infos about books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(books_info)\n",
    "df_books_info = pd.DataFrame(books_info)\n",
    "print(df_books_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the topic-document matrix to a format that pyLDAvis understands\n",
    "\n",
    "data = {}\n",
    "num_docs = len(books)\n",
    "for i in range(0, num_topics):\n",
    "    \n",
    "    # get score for all documents\n",
    "    _, document_scores, document_ids = top2vec_model.search_documents_by_topic(topic_num=i, num_docs=topic_sizes[i], reduced=reduced ) #num_docs=num_docs\n",
    "    \n",
    "    # iterate doc ids and their score for topic number i\n",
    "    for score, doc_id in zip(document_scores, document_ids):\n",
    "\n",
    "        # get book title \n",
    "        book_title = books_info[doc_id]['title']\n",
    "        \n",
    "        # if book is in keys\n",
    "        if book_title in data.keys():\n",
    "            # add score to topic number i\n",
    "            data[book_title][i] += score \n",
    "        else:\n",
    "            # create list \n",
    "            data[book_title] = [0]*num_topics   \n",
    "            data[book_title][i] += score     \n",
    "\n",
    "df_dict = {\n",
    "        'document': data.keys(),\n",
    "        'topic_contributions': data.values()\n",
    "        }             \n",
    "\n",
    "#df = pd.DataFrame.from_dict(data, orient = 'columns')\n",
    "df = pd.DataFrame.from_dict(df_dict, orient = 'columns')\n",
    "\n",
    "td_dict = {}\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    topic_distribution = row['topic_contributions']\n",
    "    document_name = row['document']\n",
    "    count =  len(df_books_info[df_books_info['title'] == document_name] )\n",
    "    td_dict[document_name] = [(i/count) for i in topic_distribution]\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "print(td_dict)\n",
    "df_save = pd.DataFrame.from_dict(td_dict)\n",
    "df_save.to_excel(\"data\\\\topics\\\\top2vec\\\\books_topic_cosine_distance_{date}.xlsx\".format(date = year))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(top2vec_model.doc_top)\n",
    "\n",
    "ret = top2vec_model.hierarchical_topic_reduction(num_topics=num_topics)\n",
    "\n",
    "doc_top_reduced = [0] * len(books_info)\n",
    "for topic_num in range(num_topics): \n",
    "    documents, document_scores, document_ids = top2vec_model.search_documents_by_topic(topic_num=topic_num, num_docs=topic_sizes[topic_num], reduced=reduced)\n",
    "    print(document_ids)\n",
    "    for document_id in document_ids:\n",
    "        doc_top_reduced[document_id] = topic_num +1\n",
    "        books_info[document_id]['topic'] = topic_num +1\n",
    "doc_top_reduced = np.array(doc_top_reduced)    \n",
    "\n",
    "df_books_info = pd.DataFrame(books_info)\n",
    "df_books_info.to_excel('data\\\\books info\\\\books_info_{date}.xlsx'.format(date = year))\n",
    "\n",
    "print(type(top2vec_model.doc_top))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_books_info = pd.DataFrame(books_info).drop_duplicates() \n",
    "df_books_info.set_index('title')\n",
    "\n",
    "\n",
    "df_save_T = df_save.transpose()\n",
    "\n",
    "df_books_info_distance = df_books_info.merge(df_save_T, left_on='title', right_index = True, how = 'outer' )\n",
    "df_books_info_distance.to_excel(\"data\\\\topics\\\\top2vec\\\\books_info_topic_distance_{date}.xlsx\".format(date = year))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 10\n",
    "\n",
    "top2vec_model = Top2Vec.load(save_path)\n",
    "ret = top2vec_model.hierarchical_topic_reduction(num_topics=num_topics)\n",
    "\n",
    "\n",
    "# # Get the document-topic vectors (embeddings)\n",
    "# embeddings = top2vec_model.get_documents_topics(doc_ids=list(range(0, len(books))))\n",
    "\n",
    "umap_args_model = {\n",
    "\"n_neighbors\": 200,\n",
    "\"n_components\": 2,\n",
    "\"metric\": \"cosine\",\n",
    "'min_dist':0.5,\n",
    "'spread':1,\n",
    "'random_state': 42\n",
    "}\n",
    "umap_model = umap.UMAP(**umap_args_model).fit(top2vec_model.document_vectors)\n",
    "\n",
    "umap.plot.points(umap_model, labels=doc_top_reduced ) #\n",
    "\n",
    "\n",
    "#plt.legend([])\n",
    "\n",
    "plt.title('Topics 2010 - 2018')\n",
    "\n",
    "# plt.xlabel('UMAP Dimension 1')\n",
    "# plt.ylabel('UMAP Dimension 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top2vec_model.generate_topic_wordcloud(7, background_color='white', reduced=reduced)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
